---
title: "Data Sources"
format: html
---

# Data sources: 


## FRED – gather real GDP from 1929 to 2023. 

* We will create a function to get the annual percentage change in real GDP per year to analyse whether the economy is in boom or recession. Chose this because it has data on America dating back to 1929 which allows us to look at a larger time period than other APIs. 
* Does not need a key 
* Rate limit: 120 requests per minute 
* Json format 
* Endpoints: Includes endpoints for series data, categories, sources, and economic releases. 
* Numerical time-series data related to economic metrics like GDP, unemployment, interest rates, inflation, etc. 
* Highly structured and easy to parse 
* Processing needs: Normalization- Time-series data may need transformation for analytics (e.g., seasonally adjusted data). If there is a range of years, we will change the data to only contain the start year for ease of analysis. (This will also be done with the other two APIs) 

## Reddit – scrape data searching social movement tags e.g ‘black lives matter’. 
Look at the frequency of tags of a social movement and check whether this trends with the emergence of new art themes or the frequency of art themes being created. Chose this as it has social movement tags organised into subreddits so we can find information on social movements easily. Reddit is also older than Twitter so we can look at a more comprehensive data set. 
Needs an access key- create an application in Reddit's developer portal. This process provides you with a client ID and a client secret, which are used to authenticate requests. 
Request limit: 60 per minute per user but can be expanded depending on usage and agreement with Reddit. We may need to design our system to stay within these limits or implement rate limiting to avoid getting temporarily blocked. 
Json format 
Endpoints: hot, new, top, and search endpoints for post data. Comment and user profile endpoints. 
User-generated content, including posts, comments, upvotes, and metadata. 
Highly unstructured text data. Data can be noisy. 
Data processing: Natural Language Processing (NLP)- For analysing themes, sentiment, and keyword extraction. It might have to be used depending on the volume of data we extract; Filtering: Removing irrelevant posts, spam, or non-English content. 
   
## Smithsonian Institution API
We will scrape data of all American art from 1929 to 2023 to look at the themes of this art. We will use this for both the first and second question. Chose this because it has a lot of data on American art ( with 11 million artworks) and it clearly categorises its art into topics (aka themes) therefore allowing us to analyse this data. 
Requires an access key: obtain by registering through the api.data.gov portal​ , GitHub , Postman API Platform . 
There is no strict request limit for the Smithsonian API, but it encourages developers to use the service responsibly and avoid excessive requests that might disrupt its systems.  
Json format 
Metadata about artifacts and artworks in the Smithsonian collection. 
Well organised and easy to query. Medium ease of use – rich but nuanced data. 
Endpoints: Object search (e.g., by keyword, themes, or artist).; Fetching metadata for specific objects.; Retrieving media (e.g., images, videos). 
Data processing: Extract and standardize themes, artists, and periods. 